{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%writefile is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/ecb_scraper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/ecb_scraper.py\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import requests\n",
    "from aiofiles import open as aio_open\n",
    "from aiohttp import ClientError\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "class ECBScraper:\n",
    "    \"\"\"\n",
    "    ECB Press Release Scraper with verbosity control\n",
    "    - verbose=0: no printing\n",
    "    - verbose=1: basic progress\n",
    "    - verbose=2: full printing (success/failure for each article)\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = \"https://www.ecb.europa.eu\"\n",
    "    START_URL = (\n",
    "        \"https://www.ecb.europa.eu/press/pubbydate/html/index.en.html?\"\n",
    "        \"name_of_publication=Press%20release\"\n",
    "    )\n",
    "\n",
    "    def __init__(self, pickle_path=\"ecb_press_releases_df.pkl\",\n",
    "                 scroll_pause_time=0.1, scroll_increment=50,\n",
    "                 max_scroll_attempts=None, initial_wait=10, verbose=1):\n",
    "        self.pickle_path = pickle_path\n",
    "        self.scroll_pause_time = scroll_pause_time\n",
    "        self.scroll_increment = scroll_increment\n",
    "        self.max_scroll_attempts = max_scroll_attempts\n",
    "        self.initial_wait = initial_wait\n",
    "        self.verbose = verbose\n",
    "        self.df = pd.DataFrame(columns=[\"Date\", \"Title\", \"URL\"])\n",
    "        self.existing_urls = set()\n",
    "        self._load_data()\n",
    "\n",
    "    # ------------------ Logging helpers ------------------\n",
    "    def _log(self, msg, level=1):\n",
    "        if self.verbose >= level:\n",
    "            print(msg)\n",
    "\n",
    "    def _log_progress(self, current, total):\n",
    "        if self.verbose >= 1:\n",
    "            print(f\"üìÑ Scraped {current}/{total} articles\")\n",
    "\n",
    "    # ------------------ Pickle management -----------------\n",
    "    def _load_data(self):\n",
    "        if os.path.exists(self.pickle_path):\n",
    "            try:\n",
    "                self.df = pd.read_pickle(self.pickle_path)\n",
    "                if not isinstance(self.df, pd.DataFrame):\n",
    "                    raise ValueError(\"Pickle content invalid.\")\n",
    "                self.existing_urls = set(self.df[\"URL\"].unique())\n",
    "                self._log(f\"‚úÖ Loaded {len(self.df)} existing articles.\", level=1)\n",
    "            except Exception as e:\n",
    "                self._log(f\"‚ö†Ô∏è Error loading pickle: {e}. Starting fresh.\", level=1)\n",
    "                self.df = pd.DataFrame(columns=[\"Date\", \"Title\", \"URL\"])\n",
    "                self.existing_urls = set()\n",
    "        else:\n",
    "            self._log(f\"‚ÑπÔ∏è No pickle found, starting fresh.\", level=1)\n",
    "            self.df = pd.DataFrame(columns=[\"Date\", \"Title\", \"URL\"])\n",
    "            self.existing_urls = set()\n",
    "\n",
    "    def _save_data(self):\n",
    "        if not self.df.empty:\n",
    "            self.df.to_pickle(self.pickle_path)\n",
    "            self._log(f\"üíæ Saved {len(self.df)} articles ‚Üí {self.pickle_path}\", level=1)\n",
    "\n",
    "    # ------------------ Scraping --------------------------\n",
    "    def _setup_driver(self):\n",
    "        try:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            service = ChromeService(ChromeDriverManager().install())\n",
    "            return webdriver.Chrome(service=service, options=options)\n",
    "        except WebDriverException as e:\n",
    "            self._log(f\"‚ùå WebDriver setup error: {e}\", level=1)\n",
    "            return None\n",
    "\n",
    "    def _scroll_page(self, driver):\n",
    "        self._log(\"üìú Scrolling page...\", level=1)\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            last_scroll_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "            driver.execute_script(f\"window.scrollBy(0, {self.scroll_increment});\")\n",
    "            time.sleep(self.scroll_pause_time)\n",
    "            new_scroll_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "            if new_scroll_position == last_scroll_position:\n",
    "                self._log(\"‚úÖ Reached page bottom.\", level=1)\n",
    "                break\n",
    "            attempt += 1\n",
    "            if self.max_scroll_attempts and attempt >= self.max_scroll_attempts:\n",
    "                self._log(\"‚ö†Ô∏è Max scroll attempts reached.\", level=1)\n",
    "                break\n",
    "\n",
    "    def _extract_articles(self, html_content):\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        main_content = soup.find(\"div\", id=\"ecb-content-col\") or soup.find(\"main\")\n",
    "        if not main_content:\n",
    "            self._log(\"‚ö†Ô∏è Could not find main content section.\", level=1)\n",
    "            return []\n",
    "\n",
    "        articles = []\n",
    "        current_date = None\n",
    "        sort_wrapper = main_content.find(\"div\", class_=\"sort-wrapper\")\n",
    "        if not sort_wrapper:\n",
    "            self._log(\"‚ö†Ô∏è No sort-wrapper found.\", level=1)\n",
    "            return []\n",
    "\n",
    "        dl = sort_wrapper.find(\"dl\", recursive=False)\n",
    "        if not dl:\n",
    "            self._log(\"‚ö†Ô∏è No <dl> in sort-wrapper.\", level=1)\n",
    "            return []\n",
    "\n",
    "        for tag in dl.find_all([\"dt\", \"dd\"], recursive=False):\n",
    "            if tag.name == \"dt\":\n",
    "                current_date = tag.get_text(strip=True)\n",
    "            elif tag.name == \"dd\" and current_date:\n",
    "                cat_div = tag.find(\"div\", class_=\"category\")\n",
    "                title_div = tag.find(\"div\", class_=\"title\")\n",
    "                if not (cat_div and title_div):\n",
    "                    continue\n",
    "                if cat_div.get_text(strip=True) != \"Press release\":\n",
    "                    continue\n",
    "                link_tag = title_div.find(\"a\", href=True)\n",
    "                if not link_tag:\n",
    "                    continue\n",
    "                url = urljoin(self.BASE_URL, link_tag[\"href\"])\n",
    "                if \"/press/pr/\" not in url:\n",
    "                    continue\n",
    "                title = link_tag.get_text(strip=True)\n",
    "                articles.append({\"Date\": current_date, \"Title\": title, \"URL\": url})\n",
    "        return articles\n",
    "\n",
    "    def scrape_and_update(self):\n",
    "        driver = self._setup_driver()\n",
    "        if not driver:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self._log(f\"üåê Navigating to {self.START_URL}\", level=1)\n",
    "            driver.get(self.START_URL)\n",
    "            time.sleep(self.initial_wait)\n",
    "            self._scroll_page(driver)\n",
    "            html = driver.page_source\n",
    "        except Exception as e:\n",
    "            self._log(f\"‚ùå Error scraping: {e}\", level=1)\n",
    "            return\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "        articles = self._extract_articles(html)\n",
    "        self._log(f\"üì∞ Found {len(articles)} articles total.\", level=1)\n",
    "        new_articles = [a for a in articles if a[\"URL\"] not in self.existing_urls]\n",
    "\n",
    "        if not new_articles:\n",
    "            self._log(\"‚ÑπÔ∏è No new articles found.\", level=1)\n",
    "            return\n",
    "\n",
    "        new_df = pd.DataFrame(new_articles)\n",
    "        self.df = pd.concat([new_df, self.df]).drop_duplicates(\"URL\", keep=\"first\").reset_index(drop=True)\n",
    "        self.existing_urls.update(new_df[\"URL\"])\n",
    "        self._save_data()\n",
    "        self._log(f\"‚úÖ Added {len(new_articles)} new articles.\", level=1)\n",
    "\n",
    "\n",
    "    # ------------------ Async fetching -------------------\n",
    "    async def _fetch_article(self, session, url, retries=3):\n",
    "        \"\"\"\n",
    "        Fetch the text content of one article from its URL asynchronously.\n",
    "        Preserves the order of <h1>, <h3>, <p>, <ul>/<ol> and relevant <div> across <main>,\n",
    "        while ignoring full divs like 'address-box -top-arrow' and 'see-also-boxes'.\n",
    "        Avoids duplicated text.\n",
    "        \"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                async with session.get(url, timeout=20) as resp:\n",
    "                    if resp.status != 200:\n",
    "                        raise aiohttp.ClientError(f\"Status {resp.status}\")\n",
    "                    html = await resp.text()\n",
    "                    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                    main = soup.find(\"main\")\n",
    "                    if not main:\n",
    "                        return \"\"\n",
    "\n",
    "                    # Remove unwanted sections\n",
    "                    for div in main.find_all(\"div\", class_=[\"address-box\", \"-top-arrow\", \"see-also-boxes\"]):\n",
    "                        div.decompose()\n",
    "\n",
    "                    content = []\n",
    "                    seen_text = set()\n",
    "\n",
    "                    # Add main headline <h1> first\n",
    "                    h1 = main.find(\"h1\", class_=\"ecb-pressContentTitle\")\n",
    "                    if h1:\n",
    "                        text = h1.get_text(strip=True)\n",
    "                        if text:\n",
    "                            content.append(text)\n",
    "                            seen_text.add(text)\n",
    "\n",
    "                    # Traverse all sections individually\n",
    "                    for section in main.find_all(\"div\", class_=\"section\"):\n",
    "                        for element in section.find_all([\"h3\", \"p\", \"ul\", \"ol\"], recursive=True):\n",
    "                            if element.name in [\"h3\", \"p\"]:\n",
    "                                text = element.get_text(strip=True)\n",
    "                                if text and text not in seen_text:\n",
    "                                    content.append(text)\n",
    "                                    seen_text.add(text)\n",
    "                            elif element.name in [\"ul\", \"ol\"]:\n",
    "                                for li in element.find_all(\"li\"):\n",
    "                                    li_text = li.get_text(strip=True)\n",
    "                                    if li_text and li_text not in seen_text:\n",
    "                                        content.append(f\"‚Ä¢ {li_text}\")\n",
    "                                        seen_text.add(li_text)\n",
    "\n",
    "                    return \"\\n\\n\".join(content).strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                wait = 2 ** attempt + random.uniform(0, 0.5)\n",
    "                self._log(f\"‚ö†Ô∏è Error fetching {url} ({e}), retrying in {wait:.1f}s...\", level=2)\n",
    "                await asyncio.sleep(wait)\n",
    "\n",
    "        self._log(f\"‚ùå Failed to fetch {url} after {retries} retries.\", level=2)\n",
    "        return \"\"\n",
    "\n",
    "    async def _save_article(self, semaphore, session, row, folder):\n",
    "        \"\"\"\n",
    "        Download and save one article with concurrency control.\n",
    "        Does not track internal counters, just saves the file.\n",
    "        \"\"\"\n",
    "        async with semaphore:\n",
    "            title, url = row.Title, row.URL\n",
    "\n",
    "            # Sanitize the title\n",
    "            safe_title = re.sub(r\"[^a-zA-Z0-9 _-]\", \"_\", title)\n",
    "            if len(safe_title) > 240:\n",
    "                url_hash = hashlib.md5(url.encode()).hexdigest()[:8]\n",
    "                safe_title = f\"{safe_title[:230]}_{url_hash}\"\n",
    "\n",
    "            file_path = os.path.join(folder, f\"{safe_title}.txt\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                if self.verbose == 2:\n",
    "                    self._log(f\"‚è© Skipping '{title}' (already saved)\", level=2)\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                text = await self._fetch_article(session, url)\n",
    "                if not text:\n",
    "                    if self.verbose >= 2:\n",
    "                        self._log(f\"‚ö†Ô∏è Empty content for '{title}'\", level=2)\n",
    "                    return\n",
    "\n",
    "                async with aio_open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    await f.write(text)\n",
    "\n",
    "                if self.verbose == 2:\n",
    "                    self._log(f\"‚úÖ Saved '{title}'\", level=2)\n",
    "\n",
    "            except Exception as e:\n",
    "                if self.verbose >= 2:\n",
    "                    self._log(f\"‚ö†Ô∏è Error saving '{title}': {e}\", level=2)\n",
    "\n",
    "\n",
    "    async def scrape_all_texts_to_files_async(self, folder=\"ecb_press_release\", concurrency=6):\n",
    "        \"\"\"\n",
    "        Scrape all articles in self.df and save them as text files asynchronously.\n",
    "        The final count of saved articles is computed by counting files in the folder.\n",
    "        \"\"\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        n = len(self.df)\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "                        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                        \"Chrome/118.0 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        semaphore = asyncio.Semaphore(concurrency)\n",
    "        connector = aiohttp.TCPConnector(limit_per_host=concurrency, force_close=True)\n",
    "\n",
    "        async with aiohttp.ClientSession(headers=headers, connector=connector) as session:\n",
    "            tasks = [self._save_article(semaphore, session, row, folder)\n",
    "                    for row in self.df.itertuples(index=False)]\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "        # Count saved files in folder\n",
    "        saved_count = len([f for f in os.listdir(folder) if f.endswith(\".txt\")])\n",
    "        self._log(f\"üìÑ Scraped {saved_count}/{n} articles successfully\", level=1)\n",
    "        self._log(f\"üéâ All texts attempted to save in '{folder}/'\", level=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from utils.ecb_scraper import ECBScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 2142 existing articles.\n",
      "üìÑ Scraped 2000/2142 articles successfully\n",
      "üéâ All texts attempted to save in 'ecb_press_release/'\n"
     ]
    }
   ],
   "source": [
    "scraper = ECBScraper(pickle_path=\"ecb_press_releases_df.pkl\", verbose=1)\n",
    "\n",
    "if scraper.df.empty:\n",
    "    scraper.scrape_and_update()\n",
    "\n",
    "await scraper.scrape_all_texts_to_files_async(concurrency=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÄúMergers and acquisitions involving the EU banking industry - facts and implications‚Äù, ‚ÄúEU banks' margins and credit standards‚Äù\n",
      "\n",
      "The European Central Bank (ECB) is releasing two reports prepared by the Banking Supervision Committee (BSC):Mergers and acquisitions involving the EU banking industry - facts and implicationsandEU banks' margins and credit standards. Both reports were prepared in the context of the Eurosystem's task of contributing to the smooth conduct of national policies on prudential supervision and financial stability.\n",
      "\n",
      "Mergers and acquisitions involving the EU banking industry - facts and implications\n",
      "\n",
      "This report analyses the main features of the consolidation process in the EU banking sector in the period from January 1995 to June 2000. The analysis looks at the rationale for mergers and acquisitions (M&As), the way they have been carried out and their implications in terms of risks and opportunities for banks. The implications for supervisory authorities are also touched upon. The report makes use of data collected from the central banks and supervisory authorities of EU Member States. Given that these data have not been gathered on the basis of a fully harmonised statistical framework, they should be interpreted with due caution.\n",
      "\n",
      "The report finds that the bulk of M&Amp;A activity takes place between domestic banks, particularly smaller banks. There has been an increase in M&Amp;A activity in more recent years and the higher relative value of involved institutions seems to point to the involvement of larger institutions. Looking at international bank M&As, it was found that most transactions involved at least one institution from outside the European Economic Area (EEA). Some regional banking groups and conglomerates have been established, but no trend towards cross-border M&As within the EEA, the EU or the euro area could be discerned in the observation period. Consolidation took place mainly at the national level and was more pronounced in the smaller countries, as was also the case prior to the period under study, whereas some of the larger countries still have rather fragmented banking markets. Concentration has, however, increased in the larger countries as well.\n",
      "\n",
      "The formation of financial conglomerates - defined for the purposes of the report as groups of financial companies operating in different sectors of the financial industry - has been observed in most Member States, albeit to a varying extent. This finding may, however, be partly due to the broad definition of a financial conglomerate adopted in the report. The process of conglomeration is mainly bank-driven, as banks have been actively expanding into other areas, in particular asset management.\n",
      "\n",
      "As for the motives for consolidation, small institutions are found to undertake M&As mainly in order to achieve economies of scale, whereas for larger institutions strategic repositioning plays an important role.\n",
      "\n",
      "M&As entail both risks and opportunities for banks. With regard to risks, three aspects are relevant. First, M&As entail an operational risk owing to the difficulty of integrating different risk management systems as well as different accounting and control procedures. Second, the problems of reconciling the different corporate cultures, including differences in working cultures and practices among the staff and business units of the two entities, entail the risk of a loss of key employees and/or clients. Third, there is the risk of failing to achieve the expected rationalisation gains owing to the complexity of the operation as well as for other reasons, including labour market rigidities. In this context it should also be mentioned that there is a risk that the structure of financial services groups may become less transparent and their business activities more difficult for the management to control. All these factors have led many observers to conclude that M&As are not always \"successful\".\n",
      "\n",
      "With regard to opportunities, it should be noted that M&As and the possibility of a takeover can provide a healthy incentive for managers to optimise the efficiency and services of their organisation with overall benefits for the efficiency and stability of the financial sector. Mergers can represent a sensible adaptation in terms of both size and strategy to a market that has been enlarged by Economic and Monetary Union (EMU). These activities may also form part of a process of consolidation aimed at reducing overcapacity, particularly among smaller banks.\n",
      "\n",
      "Supervisory and regulatory authorities are actively involved in the M&A process. As well as the general need to continuously monitor the activities of banks, there is also a need to monitor compliance with rules, regulations and practices to ensure an adequate framework for a level playing-field. In order to ensure the sound and prudent management of an institution, the supervisory authorities may reject a merger or an acquisition if they are not satisfied with the suitability of a particular acquirer or the transparency of the new group structure. In the case of financial conglomerates, supervisory authorities monitor rules and practices in order to ensure a level playing-field for financial institutions from different sectors of the financial industry and between Member States. The development of cross-border activities and financial conglomerates creates a need for closer supervisory co-operation between Member States. Memoranda of understanding are in place to facilitate such co-operation at the bilateral level, while at the multilateral level the BSC provides a forum for the discussion of the implications for and possible responses by EU supervisory authorities. In the legislative field, work is under way in the EU to develop further the prudential regulation of financial conglomerates.\n",
      "\n",
      "EU banks' margins and credit standards\n",
      "\n",
      "This report presents an assessment by the EU central banks and supervisory authorities of the pricing behaviour and credit standards of EU banks in the period from September 1997 to September 2000. The report investigates, in particular, whether banks maintain sound lending standards and a sufficiently long-term view in an environment of significant lending growth and tightening margins, or whether they base their decisions largely on prevailing economic and asset price conditions. The report is based on data compiled by the ECB on interest rates applied by banks in their lending and deposit-taking activities. Given that these data are not fully comparable across the EU countries, they should be interpreted with due caution.\n",
      "\n",
      "The report finds that there are good reasons to argue that the observed narrowing in the lending margins of banks is indeed largely attributable to the ongoing tightening of competition. However, differences between the EU countries exist. In a number of countries, new entrants into banking have substantially intensified competition. The new entrants include banking entities established by foreign banks, insurance companies, supermarket chains and car dealers. The new entrants have also stimulated the growth of \"remote banking\". The entry patterns described in the report also suggest that regional cross-border competition is intensifying in Europe. The Nordic countries, on the one hand, and the United Kingdom and Ireland, on the other, represent the clearest examples of the integration of regional financial markets.\n",
      "\n",
      "As for the pricing standards of banks for lending, a number of cases were identified where aggressive pricing by new entrants had significantly shifted pricing in the market as a whole. However, there is no clear evidence of unsound \"cut-throat\" competition undermining risk-based pricing.\n",
      "\n",
      "With regard to the banks' other lending standards, the overall assessment is broadly satisfactory. Some concern has been expressed about loan-to-value ratios that are higher than before and about less stringent collateral requirements. In general, supervisory authorities are keeping a close watch on the lending practices of banks. Supervisory authorities stress the importance of the (on-site) examination of the risk assessment, pricing methodologies and collateral management of banks, rather than the adequacy of their interest rate margins. The adoption by banks of sound provisioning policies, efficient risk management systems, effective cost controls, adequate asset quality and adequate capital levels remain the basic preconditions for the sustainability of decreased lending margins.\n",
      "\n",
      "In conclusion, the banks' internal systems for evaluating and pricing credit risk have generally undergone significant development, especially at larger banks. There is evidence to suggest that the banks' customer-rating systems in particular are often largely based on current economic conditions rather than on the development of asset quality over the business cycle. However, there are already some examples of banks moving towards a more long-term approach that evaluates loan quality over the whole economic cycle. Supervisory authorities consider it important to encourage banks to continue enhancing their methodologies for allocating economic capital according to their particular risk profile and to continue developing their internal risk management systems.\n",
      "\n",
      "The reports will be available on the ECB's website and from the ECB's Press Division at the following address:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_ecb_article_text(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch and clean text content from an ECB press release page.\n",
    "    - Extracts <h1>, <h3>, <p>, <li> elements in reading order\n",
    "    - Skips divs like 'address-box -top-arrow' and 'see-also-boxes'\n",
    "    - Avoids duplicating titles or text\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/118.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    main = soup.find(\"main\")\n",
    "    if not main:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove unwanted sections\n",
    "    for div in main.find_all(\"div\", class_=[\"address-box\", \"-top-arrow\", \"see-also-boxes\"]):\n",
    "        div.decompose()\n",
    "\n",
    "    content = []\n",
    "    seen_text = set()  # Track text to avoid duplicates\n",
    "\n",
    "    # Handle main headline <h1> separately\n",
    "    h1 = main.find(\"h1\", class_=\"ecb-pressContentTitle\")\n",
    "    if h1:\n",
    "        text = h1.get_text(strip=True)\n",
    "        if text:\n",
    "            content.append(text)\n",
    "            seen_text.add(text)\n",
    "\n",
    "    # Traverse <section> content in order\n",
    "    section = main.find(\"div\", class_=\"section\")\n",
    "    if not section:\n",
    "        return \"\\n\\n\".join(content)\n",
    "\n",
    "    for element in section.find_all([\"h3\", \"p\", \"ul\", \"ol\"], recursive=True):\n",
    "        if element.name in [\"h3\", \"p\"]:\n",
    "            text = element.get_text(strip=True)\n",
    "            if text and text not in seen_text:\n",
    "                content.append(text)\n",
    "                seen_text.add(text)\n",
    "        elif element.name in [\"ul\", \"ol\"]:\n",
    "            for li in element.find_all(\"li\"):\n",
    "                li_text = li.get_text(strip=True)\n",
    "                if li_text and li_text not in seen_text:\n",
    "                    content.append(f\"‚Ä¢ {li_text}\")\n",
    "                    seen_text.add(li_text)\n",
    "\n",
    "    return \"\\n\\n\".join(content).strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://www.ecb.europa.eu/press/pr/date/2000/html/pr001220.en.html\"\n",
    "print(fetch_ecb_article_text(url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ml_pm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
